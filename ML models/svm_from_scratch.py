# -*- coding: utf-8 -*-
"""Svm from scratch bonus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ny4NjUw3PW91gXLGZbVcmHf_3PwJ74Mh
"""

import numpy as np
from numpy import linalg
from cvxopt import matrix, solvers
import pandas as pd
import cvxopt.solvers
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from matplotlib.colors import ListedColormap

def linear_kernel(x1, x2):
    return np.dot(x1, x2)

def polynomial_kernel(x, y, p=3):
    return (1 + np.dot(x, y)) ** p

def gaussian_kernel(x, y, sigma=5.0):
    return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))

class SVM():
    def __init__(self, kernel=gaussian_kernel, C=None):
        # Constructor to initialize SVM parameters
        self.kernel = kernel  # Kernel function to use for computing similarities
        self.C = float(C) if C is not None else None  # Regularization parameter C

    def fit(self, X, y):
        # Method to train the SVM model
        n_samples, n_features = X.shape  # Number of samples and features in the training data
        K = np.zeros((n_samples, n_samples))  # Initialize the kernel matrix

        # Compute the kernel matrix K
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = self.kernel(X[i], X[j])  # Compute the similarity between samples i and j

        # Define the optimization problem for SVM
        P = cvxopt.matrix(np.outer(y, y) * K)  # Compute the P matrix for quadratic programming
        q = cvxopt.matrix(-np.ones(n_samples))  # Compute the q vector for quadratic programming
        A = cvxopt.matrix(y, (1, n_samples), 'd')  # Constraint matrix A
        b = cvxopt.matrix(0.0)  # Constraint vector b
        solvers.options['show_progress'] = False

        # Define inequality constraints Gx <= h
        if self.C is None:
            G = cvxopt.matrix(-np.identity(n_samples))  # G matrix for hard margin SVM
            h = cvxopt.matrix(np.zeros(n_samples))  # h vector for hard margin SVM
        else:
            G = cvxopt.matrix(np.vstack((-np.identity(n_samples), np.identity(n_samples))))  # G matrix for soft margin SVM
            h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * self.C)))  # h vector for soft margin SVM

        # Solve the quadratic programming problem
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)

        # Extract Lagrange multipliers (alphas) from the solution
        a = np.ravel(solution['x'])

        # Identify support vectors
        sv = a > 1e-5  # Support vectors have non-zero Lagrange multipliers
        ind = np.arange(len(a))[sv]  # Indices of support vectors

        # Store support vectors, their corresponding labels, and Lagrange multipliers
        self.a = a[sv]
        self.sv = X[sv]
        self.sv_y = y[sv]

        # Compute the bias term (intercept)
        self.b = 0
        if len(self.a) > 0:  # Check if there are any support vectors
            for n in range(len(self.a)):
                self.b += self.sv_y[n]
                self.b -= np.sum(self.a * self.sv_y * K[ind[n], sv])
            self.b /= len(self.a)  # Normalize the bias term

    def predict(self, X):
        # Method to make predictions using the trained SVM model
        y_predict = np.zeros(len(X))
        for i in range(len(X)):
            s = 0
            for j in range(len(self.a)):
                s += self.a[j] * self.sv_y[j] * self.kernel(X[i], self.sv[j])  # Compute the decision function value
            y_predict[i] = s  # Assign the decision function value to the prediction array
        return np.sign(y_predict + self.b)  # Return the sign of the decision function value as the predicted class label

    def score(self, X, y):
        # Method to evaluate the accuracy of the trained SVM model
        predictions = self.predict(X)  # Make predictions on the test data
        return np.sum(predictions == y) / len(y)  # Compute the accuracy by comparing predicted labels with true labels

def load_data(filename):
    data = np.loadtxt(filename, skiprows=7)
    X = data[:, :2]
    y = data[:, 2].astype(int)
    return X, y

def plot_decision_boundaries(X, y, classifiers):
    plt.figure(figsize=(14, 9))
    unique_labels = np.unique(y)

    colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink']

    max_X1 = max(X[:, 0]) + 3
    max_X2 = max(X[:, 1]) + 3

    combinations = np.array([[x1, x2] for x1 in range(int(max_X1)) for x2 in range(int(max_X2))])

    for idx, label in enumerate(unique_labels):
        plt.scatter(X[y == label][:, 0], X[y == label][:, 1],
                    label=f'Class {label}', edgecolor='black',color=colors[idx],s=40)


        predictions_max_values = classifiers[idx].predict(combinations)

        plt.scatter(combinations[predictions_max_values == 1][:, 0],
            combinations[predictions_max_values == 1][:, 1], s=10,color=colors[idx])

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    plt.xlabel('f1')
    plt.ylabel('f2')
    plt.xlim(xx.min() - 1, xx.max() + 1)
    plt.ylim(yy.min() - 1, yy.max() + 1)
    plt.legend()
    plt.show()

X, y = load_data("Jain.txt")

# we have 3 classes so we need to use one-vs-all strategy for multi-class classification.
# Here, we'll use one-vs-rest strategy by training a SVM for each class.

classifiers = []
for i in range(1, 2+1):
    svm = SVM(kernel=gaussian_kernel, C=3)
    y_binary = np.where(y == i, 1, -1)
    svm.fit(X, y_binary)
    classifiers.append(svm)

plot_decision_boundaries(X, y, classifiers)

def classify(file_name, num_classes):
    X, y = load_data(file_name)

    # we have 3 classes so we need to use one-vs-all strategy for multi-class classification.

    classifiers = []
    for i in range(1, num_classes+1):
        svm = SVM(kernel=gaussian_kernel, C=3)
        y_binary = np.where(y == i, 1, -1)
        svm.fit(X, y_binary)
        classifiers.append(svm)

    plot_decision_boundaries(X, y, classifiers)
classify("Spiral.txt", 3)
classify("Flame.txt", 2)
classify("Jain.txt", 2)
classify("Pathbased.txt", 3)

classify("Compound.txt", 6)
classify("Aggregation.txt", 7)