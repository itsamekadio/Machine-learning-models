# -*- coding: utf-8 -*-
"""final_ass4_NN (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KmtxEMqD3s0ig1SwwcXJltVc8DBJYcfQ
"""

import numpy as np
import pandas as pd
from keras.datasets import mnist

def load_mnist():
    # Load MNIST dataset
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    return X_train, y_train, X_test, y_test

def preprocess_data(X_train, y_train, X_test, y_test):
    # Preprocess the data
    X_train_flat = X_train.reshape(X_train.shape[0], -1) / 255.0
    X_test_flat = X_test.reshape(X_test.shape[0], -1) / 255.0
    num_classes = len(np.unique(y_train))
    y_train_encoded = np.eye(num_classes)[y_train]
    y_test_encoded = np.eye(num_classes)[y_test]
    return X_train_flat, y_train_encoded, X_test_flat, y_test_encoded, num_classes

def initialize_weights(input_size, hidden_size, output_size):
    # Initialize weights randomly
    input_weights = np.random.randn(input_size, hidden_size)
    output_weights = np.random.randn(hidden_size, output_size)
    bias_input = np.zeros((1, hidden_size))
    bias_output = np.zeros((1, output_size))
    return input_weights, output_weights, bias_input, bias_output

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def forward_pass(input_data, input_weights, output_weights, bias_input, bias_output):
    hidden_layer_input = np.dot(input_data, input_weights) + bias_input
    hidden_layer_output = sigmoid(hidden_layer_input)

    output_layer_input = np.dot(hidden_layer_output, output_weights) + bias_output
    output_layer_output = sigmoid(output_layer_input)

    return hidden_layer_output, output_layer_output

def backpropagation(input_data, target, hidden_layer_output, output_layer_output,
                    input_weights, output_weights, bias_input, bias_output, learning_rate):
    output_error = target - output_layer_output
    output_delta = output_error * sigmoid_derivative(output_layer_output)

    hidden_error = output_delta.dot(output_weights.T)
    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)

    output_weights += hidden_layer_output.T.dot(output_delta) * learning_rate
    bias_output += np.sum(output_delta, axis=0) * learning_rate
    input_weights += input_data.T.dot(hidden_delta) * learning_rate
    bias_input += np.sum(hidden_delta, axis=0) * learning_rate

def train_neural_network(X_train, y_train, X_test, y_test, hidden_size=64, learning_rate=0.04, epochs=20):
    input_size = X_train.shape[1]
    output_size = y_train.shape[1]

    input_weights, output_weights, bias_input, bias_output = initialize_weights(input_size, hidden_size, output_size)

    for epoch in range(epochs):
        for i in range(X_train.shape[0]):
            # Forward pass
            input_data = X_train[i:i+1]
            target = y_train[i:i+1]

            hidden_layer_output, output_layer_output = forward_pass(input_data, input_weights, output_weights, bias_input, bias_output)

            # Backpropagation
            backpropagation(input_data, target, hidden_layer_output, output_layer_output,
                            input_weights, output_weights, bias_input, bias_output, learning_rate)

        # Calculate accuracy
        hidden_layer_output, output_layer_output = forward_pass(X_test, input_weights, output_weights, bias_input, bias_output)
        predictions = np.argmax(output_layer_output, axis=1)
        accuracy = np.mean(predictions == np.argmax(y_test, axis=1))

        print(f'Epoch {epoch+1}/{epochs}, Test Accuracy: {accuracy}')

X_train, y_train, X_test, y_test = load_mnist()
X_train_flat, y_train_encoded, X_test_flat, y_test_encoded, num_classes = preprocess_data(X_train, y_train, X_test, y_test)
train_neural_network(X_train_flat, y_train_encoded, X_test_flat, y_test_encoded)